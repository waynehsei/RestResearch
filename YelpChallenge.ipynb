{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.1.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.13 (default, Dec 20 2016 23:05:08)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "execfile(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "phrases = []\n",
    "phrase2frq = {}\n",
    "cuisine_name = \"Mexican\"\n",
    "with open('/Users/lakerwayne/Desktop/YelpChallenge/salient.csv', 'rb') as csvfile:\n",
    "    wordsReader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    for row in wordsReader:\n",
    "        stat = row[0].split(',')\n",
    "        words = stat[0].split('_')\n",
    "        if len(words) < 3:\n",
    "            phrases.append(stat[0])\n",
    "            phrase2frq[stat[0]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|              review|               words|\n",
      "+---+--------------------+--------------------+\n",
      "|  0|I was desperately...|[i, was, desperat...|\n",
      "|  1|This is a locally...|[this, is, a, loc...|\n",
      "|  2|Maria was super n...|[maria, was, supe...|\n",
      "|  3|Yes, sometimes th...|[yes,, sometimes,...|\n",
      "|  4|The only reason t...|[the, only, reaso...|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer, RegexTokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "folder_path = '/Users/lakerwayne/Desktop/YelpChallenge/spark/'\n",
    "cuisine_path = \"/Users/lakerwayne/Desktop/YelpChallenge/cuisines/review_\" + cuisine_name + \".txt\"\n",
    "reviews = []\n",
    "\n",
    "with open(cuisine_path, 'r') as txtfile:\n",
    "    rid = 0\n",
    "    for line in txtfile.readlines():\n",
    "        review = tuple([rid, line])\n",
    "        reviews.append(review)\n",
    "        rid += 1\n",
    "\n",
    "sentenceDataFrame = spark.createDataFrame(reviews, [\"id\", \"review\"])\n",
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
    "tokenized = tokenizer.transform(sentenceDataFrame)\n",
    "tokenized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n",
    "tokenized = remover.transform(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+\n",
      "| id|              review|               words|            filtered|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "|  0|I was desperately...|[i, was, desperat...|[desperately, nee...|\n",
      "|  1|This is a locally...|[this, is, a, loc...|[locally, owned, ...|\n",
      "|  2|Maria was super n...|[maria, was, supe...|[maria, super, ni...|\n",
      "|  3|Yes, sometimes th...|[yes,, sometimes,...|[yes,, sometimes,...|\n",
      "|  4|The only reason t...|[the, only, reaso...|[reason, even, be...|\n",
      "+---+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import struct\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "\n",
    "def cleanup_text(record):\n",
    "    text = record[3]\n",
    "    text_out = [re.sub('[^a-zA-Z0-9]','',word) for word in text]\n",
    "    return text_out\n",
    "\n",
    "# define udf with an array of tokenized words\n",
    "udf_cleantext = udf(cleanup_text , ArrayType(StringType()))\n",
    "clean_text = tokenized.withColumn(\"results\", udf_cleantext(struct([tokenized[x] for x in tokenized.columns])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|              review|               words|            filtered|             results|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|I was desperately...|[i, was, desperat...|[desperately, nee...|[desperately, nee...|\n",
      "|  1|This is a locally...|[this, is, a, loc...|[locally, owned, ...|[locally, owned, ...|\n",
      "|  2|Maria was super n...|[maria, was, supe...|[maria, super, ni...|[maria, super, ni...|\n",
      "|  3|Yes, sometimes th...|[yes,, sometimes,...|[yes,, sometimes,...|[yes, sometimes, ...|\n",
      "|  4|The only reason t...|[the, only, reaso...|[reason, even, be...|[reason, even, be...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_text.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge = clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           converted|\n",
      "+--------------------+\n",
      "|[desperately, nee...|\n",
      "|[locally_owned, m...|\n",
      "|[maria, super_nic...|\n",
      "|[yes, sometimes, ...|\n",
      "|[reason, even, be...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def words2one(text):\n",
    "    list_of_words = text[4]\n",
    "    results = list_of_words\n",
    "    idx = 0\n",
    "    num_of_change=0\n",
    "    for i, w in enumerate(list_of_words):\n",
    "        idx = i-num_of_change\n",
    "        if i==len(list_of_words)-1:\n",
    "                continue\n",
    "        combined_word = results[idx]+\"_\"+results[idx+1]\n",
    "        if combined_word in phrase2frq:\n",
    "            phrase2frq[combined_word] += 1\n",
    "            results[idx] = combined_word\n",
    "            results = results[:idx+1] + results[idx+2:]\n",
    "            num_of_change += 1\n",
    "    if num_of_change==0:\n",
    "        return []\n",
    "    return results\n",
    "\n",
    "udf_convert = udf(words2one, ArrayType(StringType()))\n",
    "ctext = merge.withColumn(\"converted\", udf_convert(struct([merge[y] for y in merge.columns])))\n",
    "ctext.select(\"converted\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=100, minCount=0, inputCol=\"converted\", outputCol=\"vectors\")\n",
    "model = word2Vec.setNumPartitions(10).fit(ctext.select(\"converted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.getVectors().where(col(\"word\").isin(phrases)).toPandas().to_csv(folder_path + cusine_name + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "round_phrases = {}\n",
    "with open('/Users/lakerwayne/Desktop/YelpChallenge/spark/Mexican_sim.csv', 'rb') as csvfile:\n",
    "    creader = csv.reader(csvfile, delimiter=' ', quotechar='|')\n",
    "    first_row = True\n",
    "    for row in creader:\n",
    "        if first_row:\n",
    "            first_row = False\n",
    "            continue\n",
    "        stats = []\n",
    "        cols = row[0].split(',')\n",
    "        rest = cols[1]\n",
    "        # iterate over stats from i=2\n",
    "        for i in range(2,len(cols)):\n",
    "            if i==2:\n",
    "                vec = round(float(cols[i][2:])*1000.0)/1000\n",
    "            elif i==len(cols)-1:\n",
    "                vec = round(float(cols[i][:-2])*1000.0)/1000\n",
    "            else:\n",
    "                vec = round(float(cols[i])*1000.0)/1000\n",
    "            stats.append(vec)\n",
    "        round_phrases[rest] = stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.mllib.clustering import KMeans\n",
    "from numpy import array\n",
    "rname = []\n",
    "data = []\n",
    "for p in round_phrases.keys():\n",
    "    rname.append(p)\n",
    "    data.append(round_phrases[p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.162  0.082 -0.142 ..., -0.168 -0.016 -0.139]\n",
      " [ 0.803 -0.065  0.832 ...,  0.302  0.035  0.654]\n",
      " [ 0.915  2.055 -0.903 ..., -1.734  1.159 -2.014]\n",
      " ..., \n",
      " [-0.347  0.795 -0.306 ...,  0.68   0.375  1.041]\n",
      " [-0.392  0.437 -0.121 ...,  1.127  0.286  1.455]\n",
      " [ 0.221 -0.223  0.286 ..., -0.049  0.076 -0.081]]\n"
     ]
    }
   ],
   "source": [
    "data = array(data).reshape(len(rname), 100)\n",
    "print data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters = 2 The average silhouette_score is : 0.273847246381\n",
      "For n_clusters = 3 The average silhouette_score is : 0.258930765624\n",
      "For n_clusters = 4 The average silhouette_score is : 0.260479562336\n",
      "For n_clusters = 5 The average silhouette_score is : 0.271219948238\n",
      "For n_clusters = 6 The average silhouette_score is : 0.274052025562\n",
      "For n_clusters = 7 The average silhouette_score is : 0.16183022316\n",
      "For n_clusters = 8 The average silhouette_score is : 0.151338495823\n",
      "For n_clusters = 9 The average silhouette_score is : 0.150432670438\n",
      "For n_clusters = 10 The average silhouette_score is : 0.142594733024\n",
      "For n_clusters = 11 The average silhouette_score is : 0.140660278944\n",
      "For n_clusters = 12 The average silhouette_score is : 0.103739604283\n",
      "For n_clusters = 13 The average silhouette_score is : 0.134331786764\n",
      "For n_clusters = 14 The average silhouette_score is : 0.134657125555\n",
      "For n_clusters = 15 The average silhouette_score is : 0.135408102742\n",
      "For n_clusters = 16 The average silhouette_score is : 0.104825007097\n",
      "For n_clusters = 17 The average silhouette_score is : 0.0772115672213\n",
      "For n_clusters = 18 The average silhouette_score is : 0.0767954560635\n",
      "For n_clusters = 19 The average silhouette_score is : 0.0713405533334\n",
      "For n_clusters = 20 The average silhouette_score is : 0.0658393889549\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "\n",
    "range_n_clusters = range(2,10)\n",
    "silhlist = {}\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    \n",
    "    model = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_ind = model.fit_predict(data)\n",
    "    \n",
    "    silhouette_avg = silhouette_score(data, cluster_ind)\n",
    "    silhlist[n_clusters] = silhouette_avg\n",
    "    print(\"For n_clusters =\", n_clusters,\n",
    "          \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "k = max(silhlist, key=silhlist.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 4\n",
    "clusterer = KMeans(n_clusters=k, random_state=10)\n",
    "cluster_label = clusterer.fit_predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_label = {label:[] for label in range(k)}\n",
    "cluster_label = cluster_label.tolist()\n",
    "for i in range(len(rname)):\n",
    "    group_by_label[cluster_label[i]].append(rname[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(k):\n",
    "    with open(\"/Users/lakerwayne/Desktop/YelpChallenge/spark/Mexican\" + str(i) + \".txt\", \"w\") as txtfile:\n",
    "        for c in group_by_label[i]:\n",
    "            txtfile.write(c.encode('ascii', 'ignore'))\n",
    "            txtfile.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
